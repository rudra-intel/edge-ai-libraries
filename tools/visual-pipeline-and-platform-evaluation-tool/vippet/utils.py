import base64
import hashlib
import json
import logging
import os
import re
import time
import uuid
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Optional

from slugify import slugify

logger = logging.getLogger("utils")


def slugify_text(text: str, max_length: int = 0) -> str:
    """
    Convert text to URL-safe slug format.

    Wrapper for the slugify library. Converts text to lowercase,
    replaces spaces and special characters with dashes, and optionally
    truncates to max_length.

    Args:
        text: The text to convert to a slug.
        max_length: Maximum length of the result. If 0, no length limit is applied.

    Returns:
        URL-safe slug string.

    Example:
        >>> slugify_text("My Test Pipeline")
        'my-test-pipeline'
        >>> slugify_text("Very Long Name Here", max_length=10)
        'very-long'
    """
    regex_pattern = r"[^-a-z0-9_]+"
    if max_length > 0:
        return slugify(text, max_length=max_length, regex_pattern=regex_pattern)
    return slugify(text, regex_pattern=regex_pattern)


def generate_unique_id(
    text: str,
    existing_names: List[str],
    prefix: Optional[str] = None,
) -> str:
    """
    Generate a unique ID from text using slugify.

    The ID is generated by slugifying the text (max 64 chars). If a prefix
    is provided, it is prepended with a dash. If the resulting ID already
    exists in the provided list, a 6-character hash suffix is appended
    until a unique ID is found.

    Args:
        text: The text to convert into a slug-based ID.
        existing_names: List of existing names to check for collisions.
        prefix: Optional prefix to prepend to the generated ID.

    Returns:
        str: A unique identifier based on the slugified text.

    Example:
        >>> generate_unique_id("My Pipeline", [])
        'my-pipeline'
        >>> generate_unique_id("Test", [], prefix="pipeline")
        'pipeline-test'
        >>> generate_unique_id("Test", ["test"], prefix=None)
        'test-a3f5d9'  # with hash suffix due to collision
    """
    # Slugify the text with max length of 64 characters
    slug = slugify(text, max_length=64)

    # Prepend prefix if provided
    if prefix:
        base_id = f"{prefix}-{slug}"
    else:
        base_id = slug

    # Check if the ID already exists
    if base_id not in existing_names:
        return base_id

    # Generate unique suffix until we find a non-existing ID
    candidate_id = base_id
    counter = 0
    while candidate_id in existing_names:
        # Generate a 6-character hash from base_id + counter + timestamp
        unique_string = f"{base_id}-{counter}-{time.time_ns()}"
        hash_suffix = hashlib.sha256(unique_string.encode()).hexdigest()[:6]
        candidate_id = f"{base_id}-{hash_suffix}"
        counter += 1

    return candidate_id


def generate_pipeline_graph_id(pipeline_graph: dict) -> str:
    """
    Generate a synthetic pipeline ID from a pipeline graph hash.

    Used when a pipeline graph is provided inline instead of referencing
    an existing pipeline by ID. The ID format is "__graph-" followed by
    a 16-character hash of the graph content.

    Args:
        pipeline_graph: Dictionary representation of the pipeline graph.

    Returns:
        Synthetic pipeline ID in format "__graph-<16-char-hash>".
    """
    # Create stable JSON string (sorted keys for consistency)
    graph_json = json.dumps(pipeline_graph, sort_keys=True)

    # Generate hash and take first 16 characters
    hash_digest = hashlib.sha256(graph_json.encode()).hexdigest()[:16]

    return f"__graph-{hash_digest}"


def generate_pipeline_description_id(pipeline_description: str) -> str:
    """
    Generate a synthetic pipeline ID from a pipeline description string hash.

    Used when a pipeline description string is provided instead of referencing
    an existing pipeline by ID. The ID format is "__description-" followed by
    a 16-character hash of the description content.

    Args:
        pipeline_description: GStreamer pipeline description string.

    Returns:
        Synthetic pipeline ID in format "__description-<16-char-hash>".
    """
    # Generate hash and take first 16 characters
    hash_digest = hashlib.sha256(pipeline_description.encode()).hexdigest()[:16]

    return f"__description-{hash_digest}"


def make_tee_names_unique(
    pipeline_str: str, pipeline_index: int, stream_index: int
) -> str:
    """
    Replace all tee names in the pipeline string with unique names based on pipeline and stream indices.

    GStreamer pipelines may contain multiple tees with different names (e.g., tee name=t0, tee name=t1).
    Each tee is referenced later in the pipeline (e.g., t0., t1.).
    This function ensures each tee gets a unique name to avoid conflicts when combining multiple pipelines.

    Args:
        pipeline_str: The GStreamer pipeline string containing tee elements.
        pipeline_index: The index of the pipeline in the run specification list.
        stream_index: The index of the stream within the pipeline.

    Returns:
        str: The pipeline string with all tee names replaced with unique identifiers.

    Example:
        Input:  "... tee name=t0 ! queue t0. ! ..."
        Output: "... tee name=t100 ! queue t100. ! ..."
    """
    logger.debug(
        f"Making tee names unique for pipeline_index={pipeline_index}, stream_index={stream_index}"
    )

    # Find all unique tee names in the pipeline using regex
    # Pattern matches "tee name=<name>" where <name> is typically t0, t1, t2, etc.
    tee_pattern = r"tee\s+name=(\w+)"
    tee_matches = re.finditer(tee_pattern, pipeline_str)

    # Collect all unique tee names
    tee_names = set()
    for match in tee_matches:
        tee_names.add(match.group(1))

    if tee_names:
        logger.debug(f"Found {len(tee_names)} unique tee(s): {sorted(tee_names)}")
    else:
        logger.debug("No tee elements found in pipeline")
        return pipeline_str

    # Replace each tee name with a unique identifier
    # Using an index in addition to digits from the original name prevents
    # collisions when multiple tees have names without digits or share
    # the same digit pattern.
    for idx, original_name in enumerate(sorted(tee_names)):
        # Extract digits from original name (e.g., "t0" -> "0", "t1" -> "1")
        original_digits = "".join(filter(str.isdigit, original_name))
        # Create unique name: t{pipeline_index}{stream_index}{idx}{original_digits}
        new_name = f"t{pipeline_index}{stream_index}{idx}{original_digits}"

        # Replace "tee name=<original_name>" with "tee name=<new_name>"
        pipeline_str = re.sub(
            rf"tee\s+name={re.escape(original_name)}(?=\s|!)",
            f"tee name={new_name}",
            pipeline_str,
        )

        # Replace references to the tee (e.g., "t0." becomes "t1000.")
        pipeline_str = re.sub(
            rf"\b{re.escape(original_name)}\.", f"{new_name}.", pipeline_str
        )

    logger.debug("Tee name replacement completed successfully")
    return pipeline_str


def generate_unique_filename(filename: str) -> str:
    """
    Generate a unique filename by appending a timestamp and suffix before the file extension.

    Args:
        filename: Original filename (e.g., "video.mp4").

    Returns:
        str: Unique filename with timestamp and suffix (e.g., "video-20231012_153045-abc123.mp4").
    """
    # Extract stem and extension
    path = Path(filename)
    stem = slugify_text(Path(path.name).stem)
    ext = path.suffix

    if not ext:
        ext = ".mp4"  # default extension

    # Generate timestamp and short unique suffix
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    suffix = uuid.uuid4().hex[:6]

    # Construct new filename
    new_filename = f"{stem}-{timestamp}-{suffix}{ext}"

    return new_filename


def is_yolov10_model(model_path: str) -> bool:
    """
    Checks if the given model path corresponds to a YOLO v10 model.

    Args:
        model_path (str): Path to the model file.

    Returns:
        bool: True if the model is a YOLO v10 model, False otherwise.
    """
    return "yolov10" in model_path.lower()


def get_current_timestamp() -> datetime:
    """
    Get current UTC timestamp as datetime object.

    Returns:
        datetime: Current UTC datetime with timezone info.

    Example:
        >>> timestamp = get_current_timestamp()
        >>> # Returns datetime object like datetime(2024, 1, 15, 10, 30, 45, 123000, tzinfo=timezone.utc)
    """
    return datetime.now(timezone.utc)


def load_thumbnail_as_base64(
    thumbnail_path: str, pipeline_name: str, base_path: Optional[str] = None
) -> Optional[str]:
    """
    Load image file and convert to base64 data URI string.

    Validates that the file exists and is a valid image (PNG, JPEG, or GIF)
    by checking file signatures. If validation fails, logs a warning and
    returns None.

    Args:
        thumbnail_path: Path to the thumbnail image file. Can be absolute or
            relative. If relative and base_path is provided, the path is
            resolved relative to base_path.
        pipeline_name: Pipeline name for logging purposes.
        base_path: Optional base directory for resolving relative thumbnail paths.
            Typically the directory containing pipeline configuration files.

    Returns:
        Data URI string in format "data:{mime_type};base64,{base64_data}" if file
        is valid image, None otherwise.

    Example:
        >>> result = load_thumbnail_as_base64("/path/to/image.png", "my-pipeline")
        >>> if result:
        ...     print(result)  # "data:image/png;base64,iVBORw0KGgo..."

        >>> # With relative path and base_path
        >>> result = load_thumbnail_as_base64(
        ...     "thumbnails/image.png",
        ...     "my-pipeline",
        ...     base_path="/app/pipelines"
        ... )
    """
    if not thumbnail_path:
        logger.warning("No thumbnail path specified for pipeline '%s'", pipeline_name)
        return None

    # Resolve relative path if base_path is provided
    resolved_path = thumbnail_path
    if base_path and not os.path.isabs(thumbnail_path):
        resolved_path = os.path.join(base_path, thumbnail_path)

    if not os.path.exists(resolved_path):
        logger.warning(
            "Thumbnail file not found for pipeline '%s': %s",
            pipeline_name,
            resolved_path,
        )
        return None

    try:
        with open(resolved_path, "rb") as f:
            image_data = f.read()

        # Detect image format and get MIME type
        # PNG: 89 50 4E 47, JPEG: FF D8 FF, GIF: 47 49 46
        mime_type: Optional[str] = None
        if image_data[:4] == b"\x89PNG":
            mime_type = "image/png"
        elif image_data[:3] == b"\xff\xd8\xff":
            mime_type = "image/jpeg"
        elif image_data[:3] == b"GIF":
            mime_type = "image/gif"

        if mime_type is None:
            logger.warning(
                "Invalid image format for pipeline '%s' thumbnail: %s",
                pipeline_name,
                resolved_path,
            )
            return None

        base64_data = base64.b64encode(image_data).decode("utf-8")
        return f"data:{mime_type};base64,{base64_data}"

    except Exception as e:
        logger.warning(
            "Failed to load thumbnail for pipeline '%s' from %s: %s",
            pipeline_name,
            resolved_path,
            str(e),
        )
        return None
